use std::os::raw::c_int;

use libhdfesse::fs;

static EXCEPTION_INFO: ::phf::Map<&'_ str, c_int> = ::phf::phf_map! {
    // TODO: actually, there exception are generated by local JVM;
    // however, we handle look for remote exceptions, that is wrong.
    // We should further extend the FsError enum (as we handle the
    // NotFound), using this map as a foundation.
    "java.io.FileNotFoundException" => libc::ENOENT,
    "org.apache.hadoop.security.AccessControlException" => libc::EACCES,
    "org.apache.hadoop.fs.UnresolvedLinkException" => libc::ENOLINK,
    "org.apache.hadoop.fs.ParentNotDirectoryException" => libc::ENOTDIR,
    "java.lang.IllegalArgumentException" => libc::EINVAL,
    "java.lang.OutOfMemoryError" => libc::ENOMEM,
    "org.apache.hadoop.hdfs.server.namenode.SafeModeException" => libc::EROFS,
    "org.apache.hadoop.fs.FileAlreadyExistsException" => libc::EEXIST,
    "org.apache.hadoop.hdfs.protocol.QuotaExceededException" => libc::EDQUOT,
    "java.lang.UnsupportedOperationException" => libc::ENOTSUP,
};

// TODO: hdfs.h detects if EINTERNAL defined or not.  It seems we have
// no such possibility, thus we define it with hdfs.h default value.
// It breaks binary compatibility in this area.
pub(crate) const EINTERNAL: c_int = 255;

pub(crate) fn get_error_code(class_name: &str) -> libc::c_int {
    EXCEPTION_INFO.get(class_name).cloned().unwrap_or(EINTERNAL)
}

pub(crate) fn set_errno_with_hadoop_error(e: fs::FsError) -> fs::FsError {
    let the_errno = match &e {
        fs::FsError::NotFound(_) => libc::ENOENT,
        fs::FsError::Rpc(r) => get_error_code(r.get_class_name()),
    };
    unsafe { libc::__errno_location().replace(the_errno) };
    e
}
